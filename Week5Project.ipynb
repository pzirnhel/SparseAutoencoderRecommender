{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Recommender Systems for Movie Rating Prediction Using An Undercomplete Sparse Autoencoder [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Self-supervised learning\n",
    "\n",
    "In week 4 of this class, we studied matrix factorization and attempted to apply the technique to find a way to represent each user based on their movie ratings, a kind of user profile, by factorizing this ratings matrix using Non-negative Matrix Factorization, in order to predict how they would rate a movie they had not rated before.\n",
    "\n",
    "This technique is part of a larger class of unsupervised learning techniques called self-supervised, whose purpose is to find a way to represent the data by learning to predict the data itself from a representation, itself learned from the data. In other words, we make up a representation distribution for the data, then generate another distribution from the representation distribution, we measure how close it is from the actual data, then modify the parameters of both made-up distributions in order to fit the data better. The representation distribution is called the latent distribution. \n",
    "Because the process involves generating made-up data resembling the actual data, it is called a generative model.\n",
    "\n",
    "Generative Pre-Training, otherwise known as GPT utilizes this framework, applying it to sequences of text. It is made of an encoder and a decoder network. The purpose of the encoder network is to infer the values of the latent variables from the data and the purpose of the decoder network is to infer the values of the “made-up” data from the values of the latent variables.\n",
    "\n",
    "### What is an autoencoder?\n",
    "\n",
    "An autoencoder is a neural network made of two networks: an encoder network and a decoder network. Its purpose is to find a latent distribution from the data that can be used to generate another distribution as close to the data distribution as possible.\n",
    "\n",
    "Neural networks can have lots of layers: then they are called deep. Or they can have just one hidden layer and are called shallow. \n",
    "\n",
    "In a typical feed forward network, the kind of network we are concerned with here, each layer is represented by a matrix of weights (W), a bias vector (b) and an activation function (f). The dimension of the bias vector is the dimension of the layer. The bias vector is actually also a vector of weights. We have seen it in regression: the weight value of the intercept. In this case the layer only had one dimension, so the bias vector was just a scalar. W has the dimensions: number of features of the previous layer by number of dimensions of the layer. The bias vector is a horizontal vector that is piled as many times as the dimension of the previous layer in order to compute a bias matrix.\n",
    "\n",
    "During inference, each layer output is computed sequentially: given a previous layer output matrix $X_{l-1}$, the output $X_l$ of the layer is:\n",
    "\n",
    "$$ X_l = f(X_{l-1}. W + matrix\\_b)$$\n",
    "\n",
    "\n",
    "Typically, the function f is applied individually to each matrix entry. It can be the identity function, which is the same as applying no function. The layer is then called “linear”. It can be 0 for any negative entry and the entry itself if positive. The latter function is called Rectified Linear otherwise known as ReLu: Rectified because like with electrical current, only the positive values are let through. Linear Units because when the value is positive the value of ReLu for the entry is no different than the value of the entry itself, like a linear layer: as if it was just multiplied by 1.\n",
    "\n",
    "So one can build a shallow autoencoder having just one hidden layer, with two layers total for the network. If the dimension of the hidden layer is more than the dimension of the input layer (the features dimension), the autoencoder is called overcomplete, because the space of the hidden layer is defined by a basis of vectors whose dimension is larger than the basis defining the input space: an overcomplete basis. Conversely, if the dimension of the hidden layer is less than that of the input layer, it is called undercomplete.\n",
    "\n",
    "### The autoencoder view of matrix factorization\n",
    "\n",
    "One can think of matrix factorization as building a shallow undercomplete autoencoder with purely linear activation functions and without bias weights. Let us call $invH$ the weight matrix of the encoder layer, and $H$ the weight matrix of the decoder layer. The network is trained to perform the identity operation on the data, so approximately: $$ I \\approx invH . H $$.\n",
    "\n",
    "Matrix factorization consists of finding two matrices $W_{train}$ and $H$, such that the data: $X_{train}$ is or approximates the matrix product of $W_{train}$ and $H$:\n",
    "\n",
    "$$X_{train} \\approx X_{train\\_approx} =W_{train}. H$$\n",
    "\n",
    "So if we train the autoencoder on $X_{train}$, we get the equality:\n",
    "\n",
    "$$X_{train\\_approx} = X_{train} . invH . H$$.\n",
    "\n",
    "With $X_{train\\_approx}$ the approximate reconstruction of $X_{train}$: the predictions of $X_{train}$. This is another way to say that: $W_{train} = X_{train} . invH$\n",
    "\n",
    "In matrix factorization, we get a transform $W_{test}$, which is then multiplied by the matrix $H$ yielding an approximation of the test data $X_{test}$.\n",
    "\n",
    "$$X_{test\\_approx} = X_{test} . invH . H = W_{test} . H$$\n",
    "\n",
    "The purpose of getting an approximation is that it is more general than $X_{test}$ itself and can therefore be used to generate missing data.\n",
    "\n",
    "To summarize what happens under the hood in matrix factorization:\n",
    "First, find a matrix $H$ and its approximate left inverse $invH$: the fit function. $H$ is called the components matrix.\n",
    "Second, use them to generate a transform of the test data: $W_{test} = X_{test} . invH$: the transform function.\n",
    "Third, generate approximate data $X_{test\\_approx}$: by computing $W_{test} . H$.\n",
    "\n",
    "### Autoencoders as an extension of matrix factorization to find a latent distribution of the data\n",
    "\n",
    "In the previous paragraph, we showed that factorizing the data matrix is equivalent to building an undercomplete shallow autoencoder without bias weights, then using the results to generate missing values.\n",
    "\n",
    "However, autoencoders do not have to be shallow, may have biases and do not have to be made with linear layers. They actually perform better as generative models (see ChatGPT) if they are not. And if they may even be overcomplete, provided certain conditions are met: their hidden layers may have more dimensions than the input layer. The purpose of making them undercomplete is to avoid trivial solutions such as having weight matrices equal to the identity matrix and biases equal to zero, but trivial solutions may be avoided in many ways.\n",
    "\n",
    "One such way is to use ReLu activation functions: it transforms about half of the outputs of a linear layer into zeros, effectively decreasing the dimension of the hidden layer by half. It makes the output of the layer sparse: having lots of zeros.\n",
    "\n",
    "In any case, the reason for using non linear activation functions is because it makes for better representations. One could show that linear networks cannot compute the exclusive OR but non linear ones can. Generally, non-linear networks offer the possibility to find richer representations. \n",
    "\n",
    "## 1. Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ids_plus = pd.read_csv('data/users.csv')\n",
    "movies_attributes = pd.read_csv('data/movies.csv')\n",
    "X_y_train = pd.read_csv('data/train.csv')\n",
    "X_y_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uID</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>accupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uID gender  age  accupation    zip\n",
       "0    1      F    1          10  48067\n",
       "1    2      M   56          16  70072\n",
       "2    3      M   25          15  55117\n",
       "3    4      M   45           7  02460\n",
       "4    5      M   25          20  55455"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_ids_plus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users=len(users_ids_plus)\n",
    "user_id2idx = dict([(users_ids_plus.uID[idx],idx) for idx in range(n_users)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mID</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>Doc</th>\n",
       "      <th>Com</th>\n",
       "      <th>Hor</th>\n",
       "      <th>Adv</th>\n",
       "      <th>Wes</th>\n",
       "      <th>Dra</th>\n",
       "      <th>Ani</th>\n",
       "      <th>...</th>\n",
       "      <th>Chi</th>\n",
       "      <th>Cri</th>\n",
       "      <th>Thr</th>\n",
       "      <th>Sci</th>\n",
       "      <th>Mys</th>\n",
       "      <th>Rom</th>\n",
       "      <th>Fil</th>\n",
       "      <th>Fan</th>\n",
       "      <th>Act</th>\n",
       "      <th>Mus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mID                        title  year  Doc  Com  Hor  Adv  Wes  Dra  Ani  \\\n",
       "0    1                    Toy Story  1995    0    1    0    0    0    0    1   \n",
       "1    2                      Jumanji  1995    0    0    0    1    0    0    0   \n",
       "2    3             Grumpier Old Men  1995    0    1    0    0    0    0    0   \n",
       "3    4            Waiting to Exhale  1995    0    1    0    0    0    1    0   \n",
       "4    5  Father of the Bride Part II  1995    0    1    0    0    0    0    0   \n",
       "\n",
       "  ...   Chi  Cri  Thr  Sci  Mys  Rom  Fil  Fan  Act  Mus  \n",
       "0 ...     1    0    0    0    0    0    0    0    0    0  \n",
       "1 ...     1    0    0    0    0    0    0    1    0    0  \n",
       "2 ...     0    0    0    0    0    1    0    0    0    0  \n",
       "3 ...     0    0    0    0    0    0    0    0    0    0  \n",
       "4 ...     0    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del users_ids_plus\n",
    "movies_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_movies=len(movies_attributes)\n",
    "movie_id2idx = dict([(movies_attributes.mID[idx],idx) for idx in range(n_movies)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uID</th>\n",
       "      <th>mID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>744</td>\n",
       "      <td>1210</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3040</td>\n",
       "      <td>1584</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1451</td>\n",
       "      <td>1293</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5455</td>\n",
       "      <td>3176</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2507</td>\n",
       "      <td>3074</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uID   mID  rating\n",
       "0   744  1210       5\n",
       "1  3040  1584       4\n",
       "2  1451  1293       5\n",
       "3  5455  3176       2\n",
       "4  2507  3074       5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y=pd.concat([X_y_train,X_y_test],ignore_index=True,axis=0)\n",
    "X_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000209 entries, 0 to 1000208\n",
      "Data columns (total 3 columns):\n",
      "uID       1000209 non-null int64\n",
      "mID       1000209 non-null int64\n",
      "rating    1000209 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 22.9 MB\n"
     ]
    }
   ],
   "source": [
    "X_y.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this work, I used the data provided in homework 3, of the present course from Pr. Geena Kim DTSA 5510, “Unsupervised Learning Algorithms”. These data consist of four .csv files containing respectively a table of user demographic data including user  IDs, a table of movie attributes including movie IDs, a training and a test table of users movie ratings. I only used the user demographics file to extract a Python dictionary assigning the Pandas DataFrame row index to the user ID for this row, for every row. I did the same thing with the file about movie attributes, only with movie IDs. For the last two files, I concatenated the sets labeled for training and testing to obtain a single DataFrame of about a million or movie ratings, organized in four columns: rating index, user ID, movie ID, rating.\n",
    "\n",
    "The first order of priority was to verify there were no invalid ratings: either not a number or out of the zero to five range. If so replace them with 3s.\n",
    "\n",
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 0 invalid ratings\n"
     ]
    }
   ],
   "source": [
    "n_invalid_counts=0\n",
    "for idx in range(len(X_y)):\n",
    "    if np.isnan(X_y.rating[idx]):\n",
    "        X_y.rating[idx]=3\n",
    "        n_invalid_count+=1\n",
    "    else:\n",
    "        if X_y.rating[idx]<0 or X_y.rating[idx]>5:\n",
    "            X_y.rating[idx]=3\n",
    "            n_invalid_counts+=1\n",
    "print('There were '+str(n_invalid_counts)+' invalid ratings')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a surprise, the course authors had done the job already. The next step was to get an idea of the range of ratings counts, e. g. how many movies a given user rates. It may be harder to reliably predict unrated movies for users not having enough ratings.\n",
    "\n",
    "### Data exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts=np.zeros([len(user_id2idx)],np.int32)\n",
    "for idx in range(len(X_y)):\n",
    "    if X_y.rating[idx]>0:\n",
    "        rating_counts[user_id2idx[X_y.uID[idx]]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 13 artists>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAD8CAYAAADQUBjBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGQRJREFUeJzt3Xu0nXV95/H3xwTwLgjBwQQMVnTEjiKmwIx3aLloS3Aqa1AqGaWLmRYd6NQLaKdq0Q7aVhyXVYcRVtFRKV4qVK2YQVDHChhuISEiAREiVKKg1jKiwHf+eH7HbI7n5ELO/p0T8n6tddZ+9u/5PWd/997P5bOfy96pKiRJktTPw2a7AEmSpO2NAUySJKkzA5gkSVJnBjBJkqTODGCSJEmdGcAkSZI6M4BJkiR1ZgCTJEnqzAAmSZLU2fzZLmBjdtttt1q8ePFslyFJkrRJV1xxxQ+qasHm9J3TAWzx4sWsWLFitsuQJEnapCTf3dy+HoKUJEnqzAAmSZLUmQFMkiSpMwOYJElSZwYwSZKkzgxgkiRJnRnAJEmSOjOASZIkdWYAkyRJ6mxOfxN+L4tP+fysPfbNp7901h5bkiTNDveASZIkdWYAkyRJ6swAJkmS1JkBTJIkqTMDmCRJUmcGMEmSpM4MYJIkSZ0ZwCRJkjozgEmSJHVmAJMkSerMACZJktSZAUySJKkzA5gkSVJnBjBJkqTODGCSJEmdbXYASzIvyVVJPtfu753ksiQ3JPnbJDu29p3a/bVt/OKR/3Fqa78+yWEz/WQkSZK2BVuyB+wkYM3I/XcBZ1TVPsBdwPGt/Xjgrqp6CnBG60eSfYFjgGcAhwMfSDJv68qXJEna9mxWAEuyCHgp8OF2P8DBwKdal3OAo9rw0nafNv6Q1n8pcG5V3VNV3wHWAgfMxJOQJEnalmzuHrD3Am8E7m/3dwV+VFX3tvvrgIVteCFwK0Ab/+PW/5ftU0wjSZK03dhkAEvy28AdVXXFaPMUXWsT4zY2zejjnZBkRZIV69ev31R5kiRJ25zN2QP2XODIJDcD5zIcenwvsHOS+a3PIuC2NrwO2BOgjX8ccOdo+xTT/FJVnVlVS6pqyYIFC7b4CUmSJM11mwxgVXVqVS2qqsUMJ9F/uaqOBS4GXt66LQPOb8MXtPu08V+uqmrtx7SrJPcG9gEun7FnIkmStI2Yv+ku03oTcG6SdwBXAWe19rOAjyZZy7Dn6xiAqlqd5DzgOuBe4MSqum8rHl+SJGmbtEUBrKouAS5pwzcxxVWMVfUz4Ohppn8n8M4tLVKSJOmhxG/ClyRJ6swAJkmS1JkBTJIkqTMDmCRJUmcGMEmSpM4MYJIkSZ0ZwCRJkjozgEmSJHVmAJMkSerMACZJktSZAUySJKkzA5gkSVJnBjBJkqTODGCSJEmdGcAkSZI6M4BJkiR1ZgCTJEnqzAAmSZLUmQFMkiSpMwOYJElSZwYwSZKkzgxgkiRJnRnAJEmSOjOASZIkdWYAkyRJ6swAJkmS1JkBTJIkqTMDmCRJUmcGMEmSpM4MYJIkSZ0ZwCRJkjozgEmSJHVmAJMkSerMACZJktSZAUySJKkzA5gkSVJnBjBJkqTODGCSJEmdbTKAJXl4ksuTXJNkdZK3t/a9k1yW5IYkf5tkx9a+U7u/to1fPPK/Tm3t1yc5bFxPSpIkaS7bnD1g9wAHV9WzgP2Aw5McBLwLOKOq9gHuAo5v/Y8H7qqqpwBntH4k2Rc4BngGcDjwgSTzZvLJSJIkbQs2GcBq8NN2d4f2V8DBwKda+znAUW14abtPG39IkrT2c6vqnqr6DrAWOGBGnoUkSdI2ZLPOAUsyL8nVwB3AcuBG4EdVdW/rsg5Y2IYXArcCtPE/BnYdbZ9imtHHOiHJiiQr1q9fv+XPSJIkaY7brABWVfdV1X7AIoa9Vk+fqlu7zTTjpmuf/FhnVtWSqlqyYMGCzSlPkiRpm7JFV0FW1Y+AS4CDgJ2TzG+jFgG3teF1wJ4AbfzjgDtH26eYRpIkabuxOVdBLkiycxt+BPCbwBrgYuDlrdsy4Pw2fEG7Txv/5aqq1n5Mu0pyb2Af4PKZeiKSJEnbivmb7sIewDntisWHAedV1eeSXAecm+QdwFXAWa3/WcBHk6xl2PN1DEBVrU5yHnAdcC9wYlXdN7NPR5Ikae7bZACrqpXAs6dov4kprmKsqp8BR0/zv94JvHPLy5QkSXro8JvwJUmSOjOASZIkdWYAkyRJ6swAJkmS1JkBTJIkqTMDmCRJUmcGMEmSpM4MYJIkSZ0ZwCRJkjozgEmSJHVmAJMkSerMACZJktSZAUySJKkzA5gkSVJnBjBJkqTODGCSJEmdGcAkSZI6M4BJkiR1ZgCTJEnqzAAmSZLUmQFMkiSpMwOYJElSZwYwSZKkzgxgkiRJnRnAJEmSOjOASZIkdWYAkyRJ6swAJkmS1JkBTJIkqTMDmCRJUmcGMEmSpM4MYJIkSZ0ZwCRJkjozgEmSJHVmAJMkSerMACZJktSZAUySJKkzA5gkSVJnmwxgSfZMcnGSNUlWJzmptT8+yfIkN7TbXVp7krwvydokK5PsP/K/lrX+NyRZNr6nJUmSNHdtzh6we4E/rqqnAwcBJybZFzgFuKiq9gEuavcBjgD2aX8nAB+EIbABbwUOBA4A3joR2iRJkrYnmwxgVXV7VV3Zhv8ZWAMsBJYC57Ru5wBHteGlwEdqcCmwc5I9gMOA5VV1Z1XdBSwHDp/RZyNJkrQN2KJzwJIsBp4NXAY8oapuhyGkAbu3bguBW0cmW9fapmuf/BgnJFmRZMX69eu3pDxJkqRtwmYHsCSPBj4NnFxVP9lY1ynaaiPtD2yoOrOqllTVkgULFmxueZIkSduMzQpgSXZgCF8fq6rPtObvt0OLtNs7Wvs6YM+RyRcBt22kXZIkabuyOVdBBjgLWFNV7xkZdQEwcSXjMuD8kfbj2tWQBwE/bocoLwQOTbJLO/n+0NYmSZK0XZm/GX2eC7wKuDbJ1a3tzcDpwHlJjgduAY5u474AvARYC9wNvBqgqu5Mchrwzdbvz6rqzhl5FpIkSduQTQawqvq/TH3+FsAhU/Qv4MRp/tfZwNlbUqAkSdJDjd+EL0mS1JkBTJIkqTMDmCRJUmcGMEmSpM4MYJIkSZ0ZwCRJkjozgEmSJHVmAJMkSerMACZJktSZAUySJKkzA5gkSVJnBjBJkqTODGCSJEmdGcAkSZI6M4BJkiR1ZgCTJEnqzAAmSZLUmQFMkiSpMwOYJElSZwYwSZKkzgxgkiRJnRnAJEmSOjOASZIkdWYAkyRJ6swAJkmS1JkBTJIkqTMDmCRJUmcGMEmSpM4MYJIkSZ0ZwCRJkjozgEmSJHVmAJMkSerMACZJktSZAUySJKkzA5gkSVJnBjBJkqTODGCSJEmdbTKAJTk7yR1JVo20PT7J8iQ3tNtdWnuSvC/J2iQrk+w/Ms2y1v+GJMvG83QkSZLmvs3ZA/Y3wOGT2k4BLqqqfYCL2n2AI4B92t8JwAdhCGzAW4EDgQOAt06ENkmSpO3NJgNYVX0VuHNS81LgnDZ8DnDUSPtHanApsHOSPYDDgOVVdWdV3QUs51dDnSRJ0nbhwZ4D9oSquh2g3e7e2hcCt470W9fapmuXJEna7sz0SfiZoq020v6r/yA5IcmKJCvWr18/o8VJkiTNBQ82gH2/HVqk3d7R2tcBe470WwTctpH2X1FVZ1bVkqpasmDBggdZniRJ0tz1YAPYBcDElYzLgPNH2o9rV0MeBPy4HaK8EDg0yS7t5PtDW5skSdJ2Z/6mOiT5BPAiYLck6xiuZjwdOC/J8cAtwNGt+xeAlwBrgbuBVwNU1Z1JTgO+2fr9WVVNPrFfU1h8yudn9fFvPv2ls/r4kiQ9FG0ygFXVK6YZdcgUfQs4cZr/czZw9hZVJ0mS9BDkN+FLkiR1ZgCTJEnqzAAmSZLUmQFMkiSpMwOYJElSZwYwSZKkzgxgkiRJnRnAJEmSOjOASZIkdWYAkyRJ6swAJkmS1JkBTJIkqTMDmCRJUmcGMEmSpM4MYJIkSZ0ZwCRJkjozgEmSJHVmAJMkSerMACZJktSZAUySJKkzA5gkSVJnBjBJkqTODGCSJEmdGcAkSZI6mz/bBWjbtviUz8/q4998+ktn9fElSXow3AMmSZLUmQFMkiSpMwOYJElSZwYwSZKkzgxgkiRJnRnAJEmSOjOASZIkdWYAkyRJ6swAJkmS1JkBTJIkqTN/ikgPabP5U0n+TJIkaTruAZMkSerMPWDSLPGHzCVp+9V9D1iSw5Ncn2RtklN6P74kSdJs6xrAkswD/ho4AtgXeEWSfXvWIEmSNNt6H4I8AFhbVTcBJDkXWApc17kOSZvgIVJJGp/eAWwhcOvI/XXAgZ1rkLSNm+vhcK7XJ2n2par6PVhyNHBYVf1+u/8q4ICqet1InxOAE9rdpwHXdyhtN+AHHR7nwZjLtYH1bY25XBvM7frmcm1gfVtjLtcG1rc15nJtM+VJVbVgczr23gO2Dthz5P4i4LbRDlV1JnBmz6KSrKiqJT0fc3PN5drA+rbGXK4N5nZ9c7k2sL6tMZdrA+vbGnO5ttnQ+yrIbwL7JNk7yY7AMcAFnWuQJEmaVV33gFXVvUleC1wIzAPOrqrVPWuQJEmabd2/iLWqvgB8offjbkLXQ55baC7XBta3NeZybTC365vLtYH1bY25XBtY39aYy7V11/UkfEmSJPlbkJIkSd1tVwEsyZ5JLk6yJsnqJCe19scnWZ7khna7S8eazk5yR5JVI23PSvKNJNcm+fskj23tOyQ5p7WvSXLqmGt7eJLLk1zTXq+3t/avJbm6/d2W5LMj07yota9O8pVx1tce7+b2elydZEVre1uS743U+JJJ0+yV5KdJXj/m2p42UsPVSX6S5OQkpyVZ2dq+lOSJrf+xrX1lkn9M8qwx13dSklXtvTq5tR3d7t+fZMmk/qe2nxC7PslhY6hnqmVhymUzyePasjExb756ZJp3t7Y1Sd6XJGOq7S+SfKu9X3+XZOeRcc9sy/DqNn8+vLXvmOTMJN9u0/7u1tY2XX0j416fpJLsNqn9N5Lcl+Tlk9of25af989EbRurL8nr2vy0Osm7W9tvJbmivW5XJDm4tT8yyefb67Y6yenjrC/JfkkunVi3JDmgtafNV2vbe7//yDTL2rx6Q5JlY6xtum3EsZPWOfcn2a+Ne0XrvzLJFyfPDw+ytum2qVOuR5IsTvL/Rur70Mi4sSwbc1pVbTd/wB7A/m34McC3GX4S6d3AKa39FOBdHWt6AbA/sGqk7ZvAC9vwa4DT2vArgXPb8COBm4HFY6wtwKPb8A7AZcBBk/p8GjiuDe/M8KsGe7X7u3d4/W4GdpvU9jbg9RuZ5tPAJzfWZwx1zgP+CXgS8NiR9v8CfKgN/ztglzZ8BHDZGOv5dWBVm4/mA/8H2Ad4OsP3710CLBnpvy9wDbATsDdwIzBvhmuaalmYctkE3jwyvAC4E9ixvYZfb6/3POAbwIvGVNuhwPw2/K6ReuYDK4Fntfu7TrxWwNuBd7Thh02ed2eyvta+J8NFT98dfaz22nyZ4Xzcl0+a5n8AHwfeP+b39sVtvtup3d+93T4beOLIfPq9NvxI4MVteEfga8ARY6zvSxP/H3gJcMnI8D8wrB8PmlhOgccDN7XbXdrwLmOqbcptxKTp/g1w08g8ecfEPNCWq7fNQG3TbVOnW48snjyPjowby7Ixl/+2qz1gVXV7VV3Zhv8ZWMPw7fxLgXNat3OAozrW9FWGjceopwFfbcPLgYlPAgU8Ksl84BHAz4GfjLG2qqqftrs7tL9fnjSY5DHAwcDEHrBXAp+pqlva9HeMq7YHK8lRDCvG3lffHgLcWFXfrarR9+xRtNe0qv6xqu5q7ZcyfE/euDwduLSq7q6qe4GvAC+rqjVVNdWXHy9lCP/3VNV3gLUMPy02Y6ZZFqZbNgt4TNu79eg23b2t/eEMG+idGObZ74+jtqr6Unvt4IHv16HAyqq6pvX7YVXd18a9Bvjvrf3+qpqRL6Wc5rUDOAN4IyPLbfM6hg8iD1hGkzwHeAJD+Jgx09T3B8DpVXVP63NHu72qqia+H3I18PAkO7V59eLW5+fAlczQMjJNfQU8tg0/jg3fWbkU+EhbP14K7JxkD+AwYHlV3dmW4+XA4WOqbbptxKhXAJ9ow2l/j2rLzGOZ9B2cD7K2KbepG1mPbMxYlo25bLsKYKOSLGb4pHUZ8ISquh2GGQrYffYqA4Y9E0e24aPZ8OW1nwL+BbgduAX4y6qaaqU7Y5LMS3I1w4p6eVVdNjL6ZcBFI4HiqcAuSS5phw6OG2dtTQFfao93wkj7a9uu9rOz4bDVo4A3MXzS6u0YNqwMSfLOJLcCxwJ/OkX/4xk+ZY/LKuAFSXZN8kiGT/V7bqT/VD8jtnCM9U2Ybtl8P0OIvA24FjiprbS/AVzMsIzcDlxYVWs61PkaNrxfTwUqyYVJrkzyRoCRQ5SntfZPJnnCuApKciTD3qNrJrUvZFh2PzSp/WHAXwFvGFdNkzwVeH6Sy5J8JclvTNHnd4GrJkLahPZa/g5w0RjrOxn4i7ac/iUwccrHdMtCz2Vkum3EqP9AW+dU1S8YAu+1DMvMvsBZM1nQpG3qxuyd5Kr2nj+/Tdt12ZgrtssAluTRDJ/+Tp60N2KueA1wYpIrGHbr/ry1HwDcBzyR4TDQHyd58jgLqar7qmo/hk+aByT59ZHRo5+wYNjN/RzgpQyfBv9bkqeOsz7guVW1P8MhuxOTvAD4IPBrwH4MG+G/an3fDpwxsleviwxfOnwkw2FPAKrqLVW1J/Ax4LWT+r+YIYC9aVw1tVDyLoZPz19kOLx470Ymmeo8qtm8hPow4GqGZWE/4P0Zzl16CkMwW8Sw8Tu4zRNjk+QtDK/dx1rTfOB5DOH6ecDLkhzS2hcBX2/z7DcYNuzjqOmRwFuYOty/F3jTyF65CX8IfKGqbp1imnGYz3Co7iCG0Hde2zsDQJJnMMyj/2l0onYE4BPA+6rqpjHW9wfAH7Xl9I/YEFimWxZ6LiPTbSMASHIgcHdVrWr3d2B4Ps9mWGZWsiFQbrUt2KbeznCKyrOB/wp8vJ2/1m3ZmEu2uwDWZsRPAx+rqs+05u+3Xci021k9dFZV36qqQ6vqOQwrmhvbqFcCX6yqX7Td9V8HuvysQ1X9iOF4/uEASXZlCISjvzq8rtX3L2338VeBsZ5IPnGoor0ef8fw26Lfb8HxfuB/seFQ2YHAu5PczPDp9s0Zvhh43I4ArqyqqQ6FfZyRwwdJngl8GFhaVT8cZ1FVdVZV7V9VL2A4xHHDRrpv8mfExmS6ZfPVDIe7q6rWAt8B/jXDnp1Lq+qnLWj/A8MGfizaida/DRxbVRMb23XAV6rqB1V1N8N5VvsDPwTuZphPYQjk+zMev8bwIe2aNr8vAq5M8q8Y1hnntvaXAx9oh+b/LcOe45sZNn7HZQZPdJ/COja8h5cD9zP8ViBJFjG8TsdV1Y2TpjsTuKGq3jvG2gCWARPbiE+yYT0y3bLQbRnZyDZiwgP2uDN8SKGqbmzz6XkM50tutWm2qdPVfc/Eeq2qrmh1P5W+y8acsV0FsPbp6ixgTVW9Z2TUBQwLG+32/N61jUqye7t9GPAnbDhUcAvDJ/q0w2kHAd8aYx0LJnYNJ3kE8Jsjj3c08Lmq+tnIJOczHFKY3z6BH8hwTsC46ntUOw9t4vDiocCqiQ128zKG3fVU1fOranFVLWbYC/DnVTVjV3ptxAP2FCbZZ2TckbTXNMleDCv8V1XVt8dd1Mh8thfw73ngCnuyC4BjkuyUZG+GE/YvH3eNTL9s3sJwXh3tUMXTGM7tuwV4YZsHdwBeyJjmwSSHM+ylPLIFrQkXAs/McNXe/FbDdW3D9/fAi1q/QxguWplxVXVtVe0+Mr+vYzhZ+p+qau+R9k8Bf1hVn62qY6tqr9b+eobznE4ZR33NZxnOIaXtKd8R+EFb53weOLWqvj46QZJ3MJyPdfIY65pwG8N7R6tz4gPKBQzhNEkOAn7cDo9fCByaZJd22sOhrW3GbWQbMdF2NHDuyCTfA/ZNMvEj0b/FDCwXG9mmTtd/QZJ5bfjJDOuRm3ouG3NKzYErAXr9MRwOKIbdr1e3v5cwXKV0EcMCdhHw+I41fYJht+wvGFaSxwMnMVxN8m3gdDZ8Ye6jGT4ZrGaYOd8w5tqeCVzVXq9VwJ+OjLsEOHyKad7QalvFsDt6nPU9meHQ2TXtNXlLa/8ow7kOKxlWlntMMe3b6HAVJMOVWz8EHjfS9un2+qxkWOksbO0fBu4amTdXjLm2r7X36hrgkNb2sjYf3sNw8vqFI/3fwvCJ9Xpm6OqzSfVMtSxMuWwyHEb5UnufVwG/19rnAf+TYeNyHfCeMda2luGcn4n360Mj/X+vzZOrgHePtD+JYc/wyvZ89hpXfZPG38wUV5UBf8OkqyBb+39kZq+CnOr12xH43+01uhI4uPX9E4ZzXa8e+dudYY9Stfd2ov33x1jf84Ar2vJxGfCc1jfAX7dl4VoeeJXfa9p8sRZ49Rhrm3Ib0fq/iGEv8OT/85/bazex3tl1Bmqbbps65XqEYW//6vaaXgn8zriXjbn85zfhS5IkdbZdHYKUJEmaCwxgkiRJnRnAJEmSOjOASZIkdWYAkyRJ6swAJkmS1JkBTJIkqTMDmCRJUmf/H/FNJWE1j0mEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_bins=13\n",
    "bin_width=max(rating_counts)//n_bins\n",
    "label_list=[str(min(rating_counts)+idx*bin_width) for idx in range(n_bins)]\n",
    "hist=np.histogram(rating_counts,bins=n_bins)\n",
    "range_values=max(rating_counts)-min(rating_counts)\n",
    "fig,ax=plt.subplots(1,1,figsize=(10,4))\n",
    "plt.bar(np.arange(n_bins),height=hist[0],align='edge',tick_label=label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies rated by user who rated the fewest movies: 20\n",
      "Number of movies rated by user who rated the most movies: 2314\n"
     ]
    }
   ],
   "source": [
    "print('Number of movies rated by user who rated the fewest movies: '+str(min(rating_counts)))\n",
    "print('Number of movies rated by user who rated the most movies: '+str(max(rating_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Range: the user who rated the fewest movies only rated 20 movies, and the one who rated the most, rated over 2000: what a binge! By far, most users only rated between 20 to 198 movies. Having 20 movies at a minimum is reassuring.\n",
    "\n",
    "### Dividing the data into training and test sets\n",
    "\n",
    "The data was already randomized, so I simply divided it into two sets: 90 % for training and 10 % for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split=9*len(X_y)//10\n",
    "X_y_train=X_y.iloc[:train_test_split,:]\n",
    "X_y_test=X_y.iloc[train_test_split:,:]\n",
    "X_y_test=X_y_test.set_index(np.arange(len(X_y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uID</th>\n",
       "      <th>mID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>744</td>\n",
       "      <td>1210</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3040</td>\n",
       "      <td>1584</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1451</td>\n",
       "      <td>1293</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5455</td>\n",
       "      <td>3176</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2507</td>\n",
       "      <td>3074</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uID   mID  rating\n",
       "0   744  1210       5\n",
       "1  3040  1584       4\n",
       "2  1451  1293       5\n",
       "3  5455  3176       2\n",
       "4  2507  3074       5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 900188 entries, 0 to 900187\n",
      "Data columns (total 3 columns):\n",
      "uID       900188 non-null int64\n",
      "mID       900188 non-null int64\n",
      "rating    900188 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 20.6 MB\n"
     ]
    }
   ],
   "source": [
    "X_y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ratings=np.zeros([n_users,n_movies],np.int32)\n",
    "n_training_samples=len(X_y_train)\n",
    "for idx in range(n_training_samples):\n",
    "    users_ratings[user_id2idx[X_y_train.uID[idx]],movie_id2idx[X_y_train.mID[idx]]]=X_y_train.rating[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uID</th>\n",
       "      <th>mID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>881</td>\n",
       "      <td>1918</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4852</td>\n",
       "      <td>2804</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5831</td>\n",
       "      <td>3360</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5077</td>\n",
       "      <td>448</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3118</td>\n",
       "      <td>1224</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    uID   mID  rating\n",
       "0   881  1918       2\n",
       "1  4852  2804       5\n",
       "2  5831  3360       4\n",
       "3  5077   448       3\n",
       "4  3118  1224       4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_y_train\n",
    "X_y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100021 entries, 0 to 100020\n",
      "Data columns (total 3 columns):\n",
      "uID       100021 non-null int64\n",
      "mID       100021 non-null int64\n",
      "rating    100021 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "X_y_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Models\n",
    "\n",
    "### Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Factorizing the ratings matrix replacing zeros with 3s with TruncatedSVD\n",
    "\n",
    "In homework 4, we attempted to use non negative matrix factorization of the user ratings matrix directly and got a root mean squared value of 2.68\n",
    "\n",
    "The root mean square error measures the average distance in units of rating between the predicted and actual values, so 2.68 is very large. The problem came from the ratings matrix coding as zero non rated movies, which is equivalent to one unit below the  worst than a rating of 1. It is the same as assuming non rated movies as worse than the worst when predicting the rating of a movie. A slightly better assumption is to assume that non rated movies have a value of 3. This is implemented in the following code.\n",
    "\n",
    "This time though, because replacing all zeros by 3s makes the matrix zeros disappear, we can use Singular Value Decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_genres=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements=np.zeros([2,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_square_error(y_pred,test_data):\n",
    "        y_true=np.array(test_data.rating)\n",
    "        return np.mean(np.sqrt((y_pred-y_true)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_matrix_v1(test_data, n_latents, ratings,uid2idx,mid2idx):\n",
    "        n_samples = len(test_data)\n",
    "        n_users,n_movies=ratings.shape\n",
    "        ratings_with_3s = np.where(ratings==0.0,3.0*np.ones([n_users,n_movies]),np.float32(ratings))\n",
    "        tsvd=TruncatedSVD(n_components=n_latents)\n",
    "        tsvd.fit(ratings_with_3s)\n",
    "        preds = np.zeros([n_samples])\n",
    "        for test_idx in range(n_samples):\n",
    "            user_rating_vector = np.reshape(ratings[uid2idx[test_data.uID[test_idx]]],[1,-1])\n",
    "            user_rating_with_3s = np.where(user_rating_vector==0.0,3*np.ones([1,n_movies]),np.float32(user_rating_vector))\n",
    "            rating_transform=tsvd.transform(user_rating_with_3s)\n",
    "            movie_idx=mid2idx[test_data.mID[test_idx]]\n",
    "            preds[test_idx]=np.matmul(rating_transform,np.reshape(tsvd.components_[:,movie_idx],[-1,1]))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for label predictions based on SVD factorization with zeros replaced by 3s\n",
      "0.8530082529115297\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE for label predictions based on SVD factorization with zeros replaced by 3s\")\n",
    "preds_SVD=predict_from_matrix_v1(X_y_test, n_genres,users_ratings,user_id2idx,movie_id2idx)\n",
    "measurements[0,0]=root_mean_square_error(preds_SVD,X_y_test)\n",
    "print(measurements[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label prediction accuracy:\n",
      "0.3191030025482178\n"
     ]
    }
   ],
   "source": [
    "print(\"Label prediction accuracy:\")\n",
    "preds_SVD_int=np.int32(preds_SVD+0.5)\n",
    "measurements[1,0]=np.mean(np.float32(np.equal(preds_SVD_int,X_y_test.rating)))\n",
    "print(measurements[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a better error than similarity based methods. This is not a surprise: by replacing zeros by 3s in the Movie Ratings matrix, we make the assumption that unrated movies are equivalent to movies rated average. This is an assumption though that is not really correct, but at least it does not consider zeros as ratings less than ones. Looking by the accuracy on the predicting the labels, there is room to go up.\n",
    "\n",
    "#### 2.2. Factorizing the ratings matrix replacing zeros with 3s with keras using an autoencoder architecture\n",
    "\n",
    "Let us try to do exactly the same thing using keras first. Then talk about undercomplete sparse autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latents=n_genres\n",
    "ratings_with_3s = np.where(users_ratings==0.0,3.0*np.ones([n_users,n_movies]),np.float32(users_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4832 samples, validate on 1208 samples\n",
      "Epoch 1/5\n",
      "4832/4832 [==============================] - 1s 187us/step - loss: 2.0186 - val_loss: 0.0714\n",
      "Epoch 2/5\n",
      "4832/4832 [==============================] - 0s 103us/step - loss: 0.0611 - val_loss: 0.0682\n",
      "Epoch 3/5\n",
      "4832/4832 [==============================] - 0s 100us/step - loss: 0.0616 - val_loss: 0.0567\n",
      "Epoch 4/5\n",
      "4832/4832 [==============================] - 0s 100us/step - loss: 0.0663 - val_loss: 0.0631\n",
      "Epoch 5/5\n",
      "4832/4832 [==============================] - 0s 103us/step - loss: 0.0586 - val_loss: 0.0575\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 18)                69894     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3883)              69894     \n",
      "=================================================================\n",
      "Total params: 139,788\n",
      "Trainable params: 139,788\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(n_latents, use_bias=False),\n",
    "                                    tf.keras.layers.Dense(n_movies, use_bias=False)])\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "history=model.fit(ratings_with_3s, ratings_with_3s, batch_size=100, epochs=5,validation_split=0.2,shuffle=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network decomposes the ratings matrix X into a matrix W and a matrix H, with W equal to X.invH. \n",
    "\n",
    "Note: in order for this to work, the correct name for the layer needs to be used: it should be 'dense_1' for the invH matrix, and 'dense_2' for H, but if the model cell is ran again, a new instance of the model changes the layer names. Please adjust the names as indicated in the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n",
      "(18, 3883)\n"
     ]
    }
   ],
   "source": [
    "[invH]=model.get_layer('dense_1').get_weights()\n",
    "[H]=model.get_layer('dense_2').get_weights()\n",
    "print(invH.shape)\n",
    "print(H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_matrix_v2(ratings_with_3s,test_data, invH, H,uid2idx,mid2idx):\n",
    "        n_samples = len(test_data)\n",
    "        preds = np.zeros([n_samples])\n",
    "        for test_idx in range(n_samples):\n",
    "            transform= np.matmul(np.reshape(ratings_with_3s[uid2idx[test_data.uID[test_idx]]],[1,-1]),invH)\n",
    "            preds[test_idx]=np.sum(transform*H[:,mid2idx[test_data.mID[test_idx]]])\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for label predictions based on matrix factorization with zeros replaced by 3s\n",
      "0.9706889002204555\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE for label predictions based on matrix factorization with zeros replaced by 3s\")\n",
    "preds_keras_model2=predict_from_matrix_v2(ratings_with_3s,X_y_test,invH,H,user_id2idx,movie_id2idx)\n",
    "measurements[0,1]=root_mean_square_error(preds_keras_model2,X_y_test)\n",
    "print(measurements[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label prediction accuracy:\n",
      "0.2659541368484497\n"
     ]
    }
   ],
   "source": [
    "print(\"Label prediction accuracy:\")\n",
    "preds_keras_model2_int=np.int32(preds_keras_model2+0.5)\n",
    "measurements[1,1]=np.mean(np.float32(np.equal(preds_keras_model2_int,X_y_test.rating)))\n",
    "print(measurements[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good as with Singular Value Decomposition probably because my algorithm to obtain invH and H is likely not as precise.\n",
    "\n",
    "#### 2.3. Factorizing the ratings matrix masking zero ratings with keras using an autoencoder architecture\n",
    "\n",
    "When computing the mse, we predict 3 for missing values. A better strategy is to predict zero error with a missing value, whatever the prediction is when training the network. This is done by changing the loss in the keras model computed by the mean over each individual prediction loss term: $(y\\_pred(i) - y\\_true(i))^2$ multiplied by zero when the prediction is missing: $$ masked\\_MSE=\\Sigma _{i=0}^{n\\_movies-1} ((y\\_pred(i) - y\\_true(i))^2)sign\\_of(y\\_true)$$\n",
    "\n",
    "Nonetheless, we have seen when factorizing the ratings matrix directly, we cannot use it as an input to the network, because then zeros are interpreted by the network as worse scores than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskedMSE(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.sign(y_true)*(y_true-y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4832 samples, validate on 1208 samples\n",
      "Epoch 1/8\n",
      "4832/4832 [==============================] - 1s 176us/step - loss: 0.1364 - val_loss: 0.0395\n",
      "Epoch 2/8\n",
      "4832/4832 [==============================] - 0s 87us/step - loss: 0.0375 - val_loss: 0.0374\n",
      "Epoch 3/8\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0365 - val_loss: 0.0354\n",
      "Epoch 4/8\n",
      "4832/4832 [==============================] - 0s 83us/step - loss: 0.0362 - val_loss: 0.0349\n",
      "Epoch 5/8\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0358 - val_loss: 0.0350\n",
      "Epoch 6/8\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0371 - val_loss: 0.0370\n",
      "Epoch 7/8\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0361 - val_loss: 0.0432\n",
      "Epoch 8/8\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0371 - val_loss: 0.0359\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 18)                69894     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3883)              69894     \n",
      "=================================================================\n",
      "Total params: 139,788\n",
      "Trainable params: 139,788\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(n_latents, use_bias=False),\n",
    "                                    tf.keras.layers.Dense(n_movies, use_bias=False)])\n",
    "model.compile(optimizer = 'adam', loss = maskedMSE)\n",
    "history=model.fit(ratings_with_3s, users_ratings, batch_size=100, epochs=8,validation_split=0.2,shuffle=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in order for this to work, the correct name for the layer needs to be used: it should be 'dense_3' for the invH matrix, and 'dense_4' for H, but if the model cell is ran again, a new instance of the model changes the layer names. Please adjust the names as indicated in the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "[invH]=model.get_layer('dense_3').get_weights()\n",
    "[H]=model.get_layer('dense_4').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for label predictions based on matrix factorization with zeros replaced by 3s\n",
      "0.7875336282557377\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE for label predictions based on matrix factorization with zeros replaced by 3s\")\n",
    "preds_keras_model3=predict_from_matrix_v2(ratings_with_3s,X_y_test,invH,H,user_id2idx,movie_id2idx)\n",
    "measurements[0,2]=root_mean_square_error(preds_keras_model3,X_y_test)\n",
    "print(measurements[0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label prediction accuracy:\n",
      "0.3732116222381592\n"
     ]
    }
   ],
   "source": [
    "print(\"Label prediction accuracy:\")\n",
    "preds_keras_model3_int=np.int32(preds_keras_model3+0.5)\n",
    "measurements[1,2]=np.mean(np.float32(np.equal(preds_keras_model3_int,X_y_test.rating)))\n",
    "print(measurements[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Better than SVD !\n",
    "\n",
    "Overall matrix factorization does not work that bad for movie rating prediction, but is still on average close to 1 rating point from the target and accuracy is at best around 40 %. This is likely due to the latent variables being non linear functions of user rating patterns, and matrix factorization only considers linear functions. We also chose the number of components to equal the number of genres. What about gender and age? Women and men, children and adult often do not like the same movies. So the number of components could have been set to 18 genres + 2. Choosing the number of components introduces another assumption about the number of latent variables. Finally, all these models are equivalent to shallow autoencoders.\n",
    "\n",
    "An alternative would be a neural network with one (or more) intermediate layers with many more units than 18 or 20, incorporating non linear activation functions and using deeper networks. \n",
    "\n",
    "### Autoencoder with non linear units to encode latent variables\n",
    "\n",
    "#### 2.4. Shallow autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latents=18\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(n_latents,activation='relu'),\n",
    "                                    tf.keras.layers.Dense(n_movies)])\n",
    "model.compile(optimizer = 'adam',loss = maskedMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4832 samples, validate on 1208 samples\n",
      "Epoch 1/10\n",
      "4832/4832 [==============================] - 1s 198us/step - loss: 0.2175 - val_loss: 0.0427\n",
      "Epoch 2/10\n",
      "4832/4832 [==============================] - 0s 90us/step - loss: 0.0386 - val_loss: 0.0369\n",
      "Epoch 3/10\n",
      "4832/4832 [==============================] - 0s 89us/step - loss: 0.0364 - val_loss: 0.0354\n",
      "Epoch 4/10\n",
      "4832/4832 [==============================] - 0s 87us/step - loss: 0.0358 - val_loss: 0.0355\n",
      "Epoch 5/10\n",
      "4832/4832 [==============================] - 0s 90us/step - loss: 0.0357 - val_loss: 0.0349\n",
      "Epoch 6/10\n",
      "4832/4832 [==============================] - 0s 93us/step - loss: 0.0357 - val_loss: 0.0350\n",
      "Epoch 7/10\n",
      "4832/4832 [==============================] - 0s 90us/step - loss: 0.0354 - val_loss: 0.0349\n",
      "Epoch 8/10\n",
      "4832/4832 [==============================] - 0s 90us/step - loss: 0.0353 - val_loss: 0.0347\n",
      "Epoch 9/10\n",
      "4832/4832 [==============================] - 0s 90us/step - loss: 0.0351 - val_loss: 0.0346\n",
      "Epoch 10/10\n",
      "4832/4832 [==============================] - 0s 90us/step - loss: 0.0350 - val_loss: 0.0358\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 18)                69912     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3883)              73777     \n",
      "=================================================================\n",
      "Total params: 143,689\n",
      "Trainable params: 143,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(ratings_with_3s, users_ratings, batch_size=100, epochs=10,validation_split=0.2,shuffle=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in order for this to work, the correct name for the layer needs to be used: it should be 'dense_5' for the invH matrix, and 'dense_6' for H, but if the model cell is ran again, a new instance of the model changes the layer names. Please adjust the names as indicated in the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "[W1,b1]=model.get_layer('dense_5').get_weights()\n",
    "[W2,b2]=model.get_layer('dense_6').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_autoencoder(ratings,test_data, W1,b1,W2,b2,uid2idx,mid2idx):\n",
    "        n_samples = len(test_data)\n",
    "        preds = np.zeros([n_samples])\n",
    "        for test_idx in range(n_samples):\n",
    "            transform= np.matmul(np.reshape(ratings[uid2idx[test_data.uID[test_idx]]],[1,-1]),W1)\n",
    "            latent=np.maximum(0.0,np.reshape(transform,[-1])+b1)\n",
    "            movie_idx=mid2idx[test_data.mID[test_idx]]\n",
    "            preds[test_idx]=np.sum(latent*W2[:,movie_idx])+b2[movie_idx]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for label predictions based on sparse undercomplete autoencoder with centered inputs\n",
      "0.7606882552580159\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE for label predictions based on sparse undercomplete autoencoder with centered inputs\")\n",
    "preds_keras_model4=predict_from_autoencoder(ratings_with_3s,X_y_test,W1,b1,W2,b2,user_id2idx,movie_id2idx)\n",
    "measurements[0,3]=root_mean_square_error(preds_keras_model4,X_y_test)\n",
    "print(measurements[0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label prediction accuracy:\n",
      "0.4058047831058502\n"
     ]
    }
   ],
   "source": [
    "print(\"Label prediction accuracy:\")\n",
    "preds_keras_model4_int=np.int32(preds_keras_model4+0.5)\n",
    "measurements[1,3]=np.mean(np.float32(np.equal(preds_keras_model4_int,X_y_test.rating)))\n",
    "print(measurements[1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad either, better than the previous one.\n",
    "\n",
    "But maybe we can get better. The number of latents is rather small: 18. Using Rectified linear units, only about half these units have an output above zero, which makes the problem even worse. Let’s try to increase the number of latent variables.\n",
    "\n",
    "However, the predict_from_autoencoder() function takes a long time to complete.\n",
    "\n",
    "As with more components, the matrices W1 and W2 become very large, which markedly increases computation time. Fortunately, there is a work around: using parallel computing with keras running on the GPU.\n",
    "\n",
    "The beauty of parallel computing, and the reason why the NVDIA stock got so high (disclaimer: I do not own any such stocks), is because the multiplication of a matrix m by n with a matrix n by p is the computation of $n^2$ inner products, which can be done at the same time by $n^2$ processors. Each inner product also requires n products, so if we have $n^3$ processors available, we can do these operations in one flop. Then we have to sum n of these products, which can be done in $\\log n$ time. In our calse $n = n\\_latents$ and there are 300,000 such matrix multiplications to do: one per sample. So, by increasing $n\\_latents$ form 18 to 150, we go from $18^3\\log 18=16857$ to $150^3\\log 150=16910894$ computations. This becomes too long to do sequentially. With a GPU, it does not matter if we have $18^3=5832$ or $150^3=3375000$ threads computing: it takes $\\log(150/18)$, less than twice the time.\n",
    "\n",
    "So we will build a keras model for the prediction function, thus harnessing the power of parallel computing.\n",
    "\n",
    "First, the following function which converts the X_y_test dataframe into a numpy array of users and movies indexed by indices rather than IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df(df,uid2idx,mid2idx):\n",
    "    n_samples=len(df)\n",
    "    conversion=np.zeros([n_samples,3],np.int32)\n",
    "    for idx in range(n_samples):\n",
    "        conversion[idx,0]=uid2idx[df.uID[idx]]\n",
    "        conversion[idx,1]=mid2idx[df.mID[idx]]\n",
    "        conversion[idx,2]=df.rating[idx]\n",
    "    return conversion\n",
    "test_np = convert_df(X_y_test,user_id2idx,movie_id2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetRating(tf.keras.layers.Layer):\n",
    "    def __init__(self,ratings):\n",
    "        super().__init__()\n",
    "        self.ratings=ratings\n",
    "        self.n_units=ratings.shape[0]\n",
    "        self.mask_base=tf.reshape(tf.constant(np.float32(np.arange(self.n_units))),[1,self.n_units])\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        one_hot=tf.cast(tf.equal(tf.tile(inputs,[1,self.n_units]),self.mask_base),tf.float32)\n",
    "        return tf.matmul(one_hot,tf.cast(self.ratings,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectMovie(tf.keras.layers.Layer):\n",
    "    def __init__(self,W2,b2):\n",
    "        super().__init__()\n",
    "        self.W2=W2\n",
    "        self.b2=b2\n",
    "        self.n_units=W2.shape[1]\n",
    "        self.mask_base=tf.reshape(tf.constant(np.float32(np.arange(self.n_units))),[1,self.n_units])\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        latent,movie_idx=inputs\n",
    "        all_preds=tf.matmul(latent,W2)+b2\n",
    "        one_hot=tf.cast(tf.equal(tf.tile(movie_idx,[1,self.n_units]),self.mask_base),tf.float32)\n",
    "        return tf.reduce_sum(all_preds*one_hot,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1=tf.keras.Input(shape=(1,))\n",
    "input2=tf.keras.Input(shape=(1,))\n",
    "x1=GetRating(ratings_with_3s)(input1)\n",
    "x2=tf.keras.layers.Dense(n_latents,activation='relu',weights=[W1,b1],trainable=False)(x1)\n",
    "outputs=SelectMovie(W2,b2)([x2,input2])\n",
    "\n",
    "predictor=tf.keras.Model(inputs=[input1,input2],outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.compile(optimizer='adam',loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=predictor.predict([test_np[:,0],test_np[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for label predictions based on sparse undercomplete autoencoder\n",
      "0.7606882705811212\n",
      "Same with previous method\n",
      "0.7606882552580159\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE for label predictions based on sparse undercomplete autoencoder\")\n",
    "print(root_mean_square_error(predictions,X_y_test))\n",
    "print(\"Same with previous method\")\n",
    "print(measurements[0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label prediction accuracy:\n",
      "0.40580478\n",
      "Same with previous method\n",
      "0.4058047831058502\n"
     ]
    }
   ],
   "source": [
    "print(\"Label prediction accuracy:\")\n",
    "predictions_int=np.int32(predictions+0.5)\n",
    "print(np.mean(np.float32(np.equal(predictions_int,X_y_test.rating))))\n",
    "print(\"Same with previous method\")\n",
    "print(measurements[1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the last two models, we will use keras to get the predictions too.\n",
    "\n",
    "First, let's simply change the number of latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latents=150\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(n_latents,activation='relu'),\n",
    "                                    tf.keras.layers.Dense(n_movies)])\n",
    "model.compile(optimizer = 'adam', loss = maskedMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4832 samples, validate on 1208 samples\n",
      "Epoch 1/10\n",
      "4832/4832 [==============================] - 2s 322us/step - loss: 0.1075 - val_loss: 0.0365\n",
      "Epoch 2/10\n",
      "4832/4832 [==============================] - 0s 103us/step - loss: 0.0395 - val_loss: 0.0410\n",
      "Epoch 3/10\n",
      "4832/4832 [==============================] - 0s 100us/step - loss: 0.0373 - val_loss: 0.0372\n",
      "Epoch 4/10\n",
      "4832/4832 [==============================] - 0s 100us/step - loss: 0.0362 - val_loss: 0.0354\n",
      "Epoch 5/10\n",
      "4832/4832 [==============================] - 0s 103us/step - loss: 0.0363 - val_loss: 0.0350\n",
      "Epoch 6/10\n",
      "4832/4832 [==============================] - 0s 101us/step - loss: 0.0359 - val_loss: 0.0349\n",
      "Epoch 7/10\n",
      "4832/4832 [==============================] - 0s 103us/step - loss: 0.0360 - val_loss: 0.0351\n",
      "Epoch 8/10\n",
      "4832/4832 [==============================] - 0s 101us/step - loss: 0.0366 - val_loss: 0.0439\n",
      "Epoch 9/10\n",
      "4832/4832 [==============================] - 0s 100us/step - loss: 0.0360 - val_loss: 0.0348\n",
      "Epoch 10/10\n",
      "4832/4832 [==============================] - 0s 103us/step - loss: 0.0353 - val_loss: 0.0347\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 150)               582600    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3883)              586333    \n",
      "=================================================================\n",
      "Total params: 1,168,933\n",
      "Trainable params: 1,168,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(ratings_with_3s, users_ratings, batch_size=100, epochs=10,validation_split=0.2,shuffle=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "[W1,b1]=model.get_layer('dense_7').get_weights()\n",
    "[W2,b2]=model.get_layer('dense_8').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1=tf.keras.Input(shape=(1,))\n",
    "input2=tf.keras.Input(shape=(1,))\n",
    "x1=GetRating(ratings_with_3s)(input1)\n",
    "x2=tf.keras.layers.Dense(n_latents,activation='relu',weights=[W1,b1],trainable=False)(x1)\n",
    "outputs=SelectMovie(W2,b2)([x2,input2])\n",
    "\n",
    "predictor=tf.keras.Model(inputs=[input1,input2],outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no trainable parameters so this actually does not matter but we still have to compile the model\n",
    "predictor.compile(optimizer='adam',loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=predictor.predict([test_np[:,0],test_np[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for label predictions based on sparse undercomplete autoencoder\n",
      "0.7602566000233147\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE for label predictions based on sparse undercomplete autoencoder\")\n",
    "measurements[0,4]=root_mean_square_error(predictions,X_y_test)\n",
    "print(measurements[0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label prediction accuracy:\n",
      "0.39364734292030334\n"
     ]
    }
   ],
   "source": [
    "print(\"Label prediction accuracy:\")\n",
    "predictions_int=np.int32(predictions+0.5)\n",
    "measurements[1,4]=np.mean(np.float32(np.equal(predictions_int,X_y_test.rating)))\n",
    "print(measurements[1,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad either, about the same. So it was not so much the size of the latent layer.\n",
    "\n",
    "Maybe be it is the depth of the network.\n",
    "\n",
    "#### 2.5 Deeper autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_e=80\n",
    "n_latents=20\n",
    "n_hidden_d=80\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(n_hidden_e,activation='relu'),\n",
    "                                    tf.keras.layers.Dense(n_latents,activation='relu'),\n",
    "                                    tf.keras.layers.Dense(n_hidden_d,activation='relu'),\n",
    "                                    tf.keras.layers.Dense(n_movies)])\n",
    "model.compile(optimizer = 'adam', loss = maskedMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4832 samples, validate on 1208 samples\n",
      "Epoch 1/10\n",
      "4832/4832 [==============================] - 2s 462us/step - loss: 0.1031 - val_loss: 0.0367\n",
      "Epoch 2/10\n",
      "4832/4832 [==============================] - 0s 101us/step - loss: 0.0373 - val_loss: 0.0356\n",
      "Epoch 3/10\n",
      "4832/4832 [==============================] - 0s 101us/step - loss: 0.0367 - val_loss: 0.0353\n",
      "Epoch 4/10\n",
      "4832/4832 [==============================] - 0s 100us/step - loss: 0.0369 - val_loss: 0.0352\n",
      "Epoch 5/10\n",
      "4832/4832 [==============================] - 0s 100us/step - loss: 0.0360 - val_loss: 0.0351\n",
      "Epoch 6/10\n",
      "4832/4832 [==============================] - 0s 100us/step - loss: 0.0358 - val_loss: 0.0349\n",
      "Epoch 7/10\n",
      "4832/4832 [==============================] - 0s 103us/step - loss: 0.0359 - val_loss: 0.0348\n",
      "Epoch 8/10\n",
      "4832/4832 [==============================] - 1s 105us/step - loss: 0.0358 - val_loss: 0.0348\n",
      "Epoch 9/10\n",
      "4832/4832 [==============================] - 0s 99us/step - loss: 0.0357 - val_loss: 0.0355\n",
      "Epoch 10/10\n",
      "4832/4832 [==============================] - 0s 100us/step - loss: 0.0350 - val_loss: 0.0347\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 80)                310720    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 80)                1680      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3883)              314523    \n",
      "=================================================================\n",
      "Total params: 628,543\n",
      "Trainable params: 628,543\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(ratings_with_3s, users_ratings, batch_size=100, epochs=10,validation_split=0.2,shuffle=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "[W1,b1]=model.get_layer('dense_11').get_weights()\n",
    "[W2,b2]=model.get_layer('dense_12').get_weights()\n",
    "[W3,b3]=model.get_layer('dense_13').get_weights()\n",
    "[W4,b4]=model.get_layer('dense_14').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectMovieDeep(tf.keras.layers.Layer):\n",
    "    def __init__(self,W4,b4):\n",
    "        super().__init__()\n",
    "        self.W4=W4\n",
    "        self.b4=b4\n",
    "        self.n_units=W4.shape[1]\n",
    "        self.mask_base=tf.reshape(tf.constant(np.float32(np.arange(self.n_units))),[1,self.n_units])\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        latent,movie_idx=inputs\n",
    "        all_preds=tf.matmul(latent,W4)+b4\n",
    "        one_hot=tf.cast(tf.equal(tf.tile(movie_idx,[1,self.n_units]),self.mask_base),tf.float32)\n",
    "        return tf.reduce_sum(all_preds*one_hot,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1=tf.keras.Input(shape=(1,))\n",
    "input2=tf.keras.Input(shape=(1,))\n",
    "x1=GetRating(ratings_with_3s)(input1)\n",
    "x2=tf.keras.layers.Dense(n_hidden_e,activation='relu',weights=[W1,b1],trainable=False)(x1)\n",
    "x3=tf.keras.layers.Dense(n_latents,activation='relu',weights=[W2,b2],trainable=False)(x2)\n",
    "x4=tf.keras.layers.Dense(n_hidden_d,activation='relu',weights=[W3,b3],trainable=False)(x3)\n",
    "outputs=SelectMovieDeep(W4,b4)([x4,input2])\n",
    "\n",
    "predictor_deep=tf.keras.Model(inputs=[input1,input2],outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no trainable parameters so this actually does not matter but we still have to compile the model\n",
    "predictor_deep.compile(optimizer='adam',loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_deep=predictor_deep.predict([test_np[:,0],test_np[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for label predictions based on sparse undercomplete deeper autoencoder\n",
      "0.7651896632900824\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE for label predictions based on sparse undercomplete deeper autoencoder\")\n",
    "measurements[0,5]=root_mean_square_error(predictions_deep,X_y_test)\n",
    "print(measurements[0,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label prediction accuracy:\n",
      "0.38740864396095276\n"
     ]
    }
   ],
   "source": [
    "print(\"Label prediction accuracy:\")\n",
    "predictions_deep_int=np.int32(predictions_deep+0.5)\n",
    "measurements[1,5]=np.mean(np.float32(np.equal(predictions_deep_int,X_y_test.rating)))\n",
    "print(measurements[1,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the same. So depth does not have an impact either.\n",
    "\n",
    "I used early stopping as a regularization technique, training up until the validation accuracy got as close as possible to the training accuracy, attempted using regularizers such as the L1 and L2 norm, but did not get better results. So it seams like the model still overfits. It was worse with a 70/30% data split. This is why I chose a more usual ratio is 10:1, thus providing more training data, and ran the experiments again. \n",
    "\n",
    "For now, let us summarize the results.\n",
    "\n",
    "## 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['Matrix Factorization: SVD','Keras','Keras Masked MSE','Shallow AE 18 latents','Shallow 150','Deeper AE']\n",
    "results=pd.DataFrame(measurements,index=['RMSE','Accuracy'],columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matrix Factorization: SVD</th>\n",
       "      <th>Keras</th>\n",
       "      <th>Keras Masked MSE</th>\n",
       "      <th>Shallow AE 18 latents</th>\n",
       "      <th>Shallow 150</th>\n",
       "      <th>Deeper AE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>0.853008</td>\n",
       "      <td>0.970689</td>\n",
       "      <td>0.787534</td>\n",
       "      <td>0.760688</td>\n",
       "      <td>0.760257</td>\n",
       "      <td>0.765190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.319103</td>\n",
       "      <td>0.265954</td>\n",
       "      <td>0.373212</td>\n",
       "      <td>0.405805</td>\n",
       "      <td>0.393647</td>\n",
       "      <td>0.387409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Matrix Factorization: SVD     Keras  Keras Masked MSE  \\\n",
       "RMSE                       0.853008  0.970689          0.787534   \n",
       "Accuracy                   0.319103  0.265954          0.373212   \n",
       "\n",
       "          Shallow AE 18 latents  Shallow 150  Deeper AE  \n",
       "RMSE                   0.760688     0.760257   0.765190  \n",
       "Accuracy               0.405805     0.393647   0.387409  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this table makes it clear that replacing zero ratings by the average 3 improved the root mean squared error for all the models. Simply replicating the factorization with keras yielded worse number, most likely because the algorithm used with keras gave worse approximations for the transform and the components. \n",
    "\n",
    "On the other hand, training the network masking unrated movies (e. g. assigning a zero loss in the training algorithm for movies the user had not rated), improved the Root Mean Square error of all models including the keras matrix factorization one.\n",
    "\n",
    "The best performance was obtained by the undercomplete sparse autoencoders although it did not matter if they were shallow or deeper, or if the number of latents was large or small. The models improved accuracy compared to SVD by 3 points. \n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This work explored using a sparse undercomplete autoencoder to find a latent distribution to fit the data and compared it to using matrix factorization. The main results was that a sparse autoencoder performed better than matrix factorization as long as zero ratings, standing for user unrated movies, were replaced by the average rating, and that the training loss assigned zero loss for movies not rated by the sample user and there was enough training data.\n",
    "\n",
    "Other factors such as the dimension of the latent vector or the depth of the network did not have much influence, probably because it makes the models overfit. Early stopping was used as a regularizer. I also attempted to use L1 and L2 penalties (not shown), but it did not help. Using the user demographic data as a covariate may also have affected the quality of the prediction. This could be done with a conditional autoencoder architecture.\n",
    "\n",
    "This assignment is about unsupervised learning, so we did not explore an obvious solution of using supervised learning: building a categorical classifier with the five ratings as the five possible categories. It may turn out to be a better strategy to predict labels. \n",
    "\n",
    "Nonetheless, this work also demonstrated the advantage of parallel processing. Using sparse matrices would have been possible with the user ratings and latent representations, but the weights matrices had to be dense, thus canceling the advantage of such sparse matrices when performing matrix multiplications. \n",
    "\n",
    "Parallel processing was really a plus in this work. Using a laptop and a Keras backend software taking advantage of GPU computing made scaling the autoencoder models up easy without a corresponding cost in execution time. Manipulating large, dense matrices, like the ratings matrix is not a big obstacle when using parallel computing. Memory is more of an issue. Still 8 GB of GPU memory on this computer was enough for the work. Regardless even then memory may be distributed too.\n",
    "\n",
    "Finally, with parallel processing, finding one rating for one user and one movie takes the same amount of time as finding about 4000 movie ratings for one user. So, ranking these ratings and displaying the first top 10 or 20 provides a way to recommend to the user their likely most wanted movies in one pass, instead of having the user request one rating for one given movie at a time. How can you resist that!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latents=18\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(n_latents,activation='relu'),\n",
    "                                    tf.keras.layers.Dense(n_movies)])\n",
    "model.compile(optimizer = 'adam',loss = maskedMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4832 samples, validate on 1208 samples\n",
      "Epoch 1/10\n",
      "4832/4832 [==============================] - 4s 733us/step - loss: 0.2028 - val_loss: 0.0416\n",
      "Epoch 2/10\n",
      "4832/4832 [==============================] - 0s 85us/step - loss: 0.0379 - val_loss: 0.0358\n",
      "Epoch 3/10\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0361 - val_loss: 0.0355\n",
      "Epoch 4/10\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0358 - val_loss: 0.0351\n",
      "Epoch 5/10\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0357 - val_loss: 0.0350\n",
      "Epoch 6/10\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0356 - val_loss: 0.0349\n",
      "Epoch 7/10\n",
      "4832/4832 [==============================] - 0s 89us/step - loss: 0.0353 - val_loss: 0.0360\n",
      "Epoch 8/10\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0353 - val_loss: 0.0347\n",
      "Epoch 9/10\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0352 - val_loss: 0.0350\n",
      "Epoch 10/10\n",
      "4832/4832 [==============================] - 0s 86us/step - loss: 0.0353 - val_loss: 0.0357\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 18)                69912     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3883)              73777     \n",
      "=================================================================\n",
      "Total params: 143,689\n",
      "Trainable params: 143,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(ratings_with_3s, users_ratings, batch_size=100, epochs=10,validation_split=0.2,shuffle=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in order for this to work, the correct name for the layer needs to be used: it should be 'dense_21' for the invH matrix, and 'dense_22' for H, but if the model cell is ran again, a new instance of the model changes the layer names. Please adjust the names as indicated in the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "[W1,b1]=model.get_layer('dense_21').get_weights()\n",
    "[W2,b2]=model.get_layer('dense_22').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict4000Movies(tf.keras.layers.Layer):\n",
    "    def __init__(self,W2,b2):\n",
    "        super().__init__()\n",
    "        self.W2=W2\n",
    "        self.b2=b2\n",
    "        self.n_units=W2.shape[1]\n",
    "        self.mask_base=tf.reshape(tf.constant(np.float32(np.arange(self.n_units))),[1,self.n_units])\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        latent,movie_idx=inputs\n",
    "        all_preds=tf.matmul(latent,W2)+b2\n",
    "        return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1=tf.keras.Input(shape=(1,))\n",
    "input2=tf.keras.Input(shape=(1,))\n",
    "x1=GetRating(ratings_with_3s)(input1)\n",
    "x2=tf.keras.layers.Dense(n_latents,activation='relu',weights=[W1,b1],trainable=False)(x1)\n",
    "outputs=Predict4000Movies(W2,b2)([x2,input2])\n",
    "\n",
    "predictor=tf.keras.Model(inputs=[input1,input2],outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.compile(optimizer='adam',loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27274 86862]\n"
     ]
    }
   ],
   "source": [
    "random_2_users=np.int32(np.floor(X_y_test.shape[0]*np.random.rand(2)))\n",
    "print(random_2_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=predictor.predict([test_np[random_2_users,0],test_np[random_2_users,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1762  568 3211 1689 3164  435 1399 2862 2411 2273]\n"
     ]
    }
   ],
   "source": [
    "random_user=random_2_users[0]\n",
    "order=np.argsort(-predictions[0,:])\n",
    "ten_best=order[:10]\n",
    "print(ten_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended for movie watcher: 2651\n",
      "Follow the Bitch\n",
      "Foreign Student\n",
      "Baby, The\n",
      "Men of Means\n",
      "Smashing Time\n",
      "Dangerous Game\n",
      "Hearts and Minds\n",
      "Time of the Gypsies (Dom za vesanje)\n",
      "Dry Cleaning (Nettoyage � sec)\n",
      "Hard Core Logo\n"
     ]
    }
   ],
   "source": [
    "print('Recommended for movie watcher: '+str(X_y_test.uID[random_user]))\n",
    "for idx in ten_best:\n",
    "    print(movies_attributes.title[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Goodfellow I, Bengio Y., Courville A. (2016). \"Autoencoders\". In \"Deep Learning\" pp. 494-495. MIT Press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
